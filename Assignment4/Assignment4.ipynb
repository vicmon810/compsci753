{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43dd812b",
   "metadata": {},
   "source": [
    "# Brute-force Algorithm \n",
    "\n",
    "### (a) Compute the average frequency of the words in the stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d4154e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def read_words (file_path):\n",
    "    article_id = []\n",
    "    words = []\n",
    "    date = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 3:  # Ensure each line has at least 3 parts\n",
    "                article_id.append(parts[0])\n",
    "                words.append(parts[1])\n",
    "                date.append(parts[2])\n",
    "    return article_id, words, date\n",
    "\n",
    "id , words, date = read_words(\"test.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Frequency of Each Word in the Stream:\n",
      "describe new algorithm k \\ell -pebble game colors use obtain characterization family k \\ell -sparse graphs algorithmic solutions family problems concerning tree decompositions graphs Special instances sparse graphs appear rigidity theory received increased attention recent years particular colored pebbles generalize strengthen previous results Lee Streinu give new proof Tutte-Nash-Williams characterization arboricity also present new decomposition certifies sparsity based k \\ell -pebble game colors work also exposes connections pebble game algorithms previous sparse graph algorithms Gabow Gabow Westermann Hendrickson: 0.1000\n",
      "show determinant Stirling cycle numbers counts unlabeled acyclic single-source automata proof involves bijection automata certain marked lattice paths sign-reversing involution evaluate determinant: 0.1000\n",
      "study two-particle wave function paired atoms Fermi gas tunable interaction strengths controlled Feshbach resonance Cooper pair wave function examined bosonic characters quantified correction Bose enhancement factor associated creation annihilation composite particle operators example given three-dimensional uniform gas Two definitions Cooper pair wave function examined One chosen reflect off-diagonal long range order ODLRO Another one corresponds pair projection BCS state side negative scattering length found paired atoms described ODLRO bosonic pair projected definition also found k_F -1 \\ge 1 definitions give similar results 90 atoms occupy corresponding molecular condensates: 0.1000\n",
      "rather non-standard quantum representation canonical commutation relations quantum mechanics systems known polymer representation gained attention recent years due possible relation Planck scale physics particular approach followed symmetric sector loop quantum gravity known loop quantum cosmology explore different aspects relation ordinary Schroedinger theory polymer description paper two parts first one derive polymer quantum mechanics starting ordinary Schroedinger theory show polymer description arises appropriate limit second part consider continuum limit theory namely reverse process one starts discrete theory tries recover back ordinary Schroedinger quantum mechanics consider several examples interest including harmonic oscillator free particle simple cosmological model: 0.1000\n",
      "general formulation developed represent material models applications dynamic loading Numerical methods devised calculate response shock ramp compression ramp decompression generalizing previous solutions scalar equations state numerical methods found flexible robust matched analytic results high accuracy basic ramp shock solution methods coupled solve composite deformation paths shock-induced impacts shock interactions planar interface different materials calculations capture much physics typical material dynamics experiments without requiring spatially-resolving simulations Example calculations made loading histories metals illustrating effects plastic work temperatures induced quasi-isentropic shock-release experiments effect phase transition: 0.1000\n",
      "Partial cubes isometric subgraphs hypercubes Structures graph defined means semicubes Djokovi\\ c 's Winkler 's relations play important role theory partial cubes structures employed paper characterize bipartite graphs partial cubes arbitrary dimension New characterizations established new proofs known results given operations Cartesian product pasting expansion contraction processes utilized paper construct new partial cubes old ones particular isometric lattice dimensions finite partial cubes obtained means operations calculated: 0.1000\n",
      "paper present algorithm computing Hecke eigensystems Hilbert-Siegel cusp forms real quadratic fields narrow class number one give illustrative examples using quadratic field \\Q \\sqrt 5 examples identify Hilbert-Siegel eigenforms possible lifts Hilbert eigenforms: 0.1000\n",
      "Recently Bruinier Ono classified cusp forms f z \\sum_ n=0 \\infty a_f n q ^n \\in S_ \\lambda+1/2 \\Gamma_0 N \\chi \\cap \\mathbb Z q satisfy certain distribution property modulo odd primes p paper using Rankin-Cohen Bracket extend result modular forms half integral weight primes p \\geq 5 applications main theorem derive distribution properties modulo primes p\\geq5 traces singular moduli Hurwitz class number also study analogue Newman 's conjecture overpartitions: 0.1000\n",
      "Serre obtained p-adic limit integral Fourier coefficient modular forms SL_2 \\mathbb Z p=2,3,5,7 paper extend result Serre weakly holomorphic modular forms half integral weight \\Gamma_ 0 4N N=1,2,4 proof based linear relations among Fourier coefficients modular forms half integral weight applications obtain congruences Borcherds exponents congruences quotient Eisentein series congruences values L -functions certain point also studied Furthermore congruences Fourier coefficients Siegel modular forms Maass Space obtained using Ikeda lifting: 0.1000\n",
      "work evaluate lifetimes doubly charmed baryons \\Xi_ cc \\Xi_ cc ++ \\Omega_ cc carefully calculate non-spectator contributions quark level Cabibbo-suppressed diagrams also included hadronic matrix elements evaluated simple non-relativistic harmonic oscillator model numerical results generally consistent obtained authors used diquark model However theoretical predictions lifetimes one order larger upper limit set recent SELEX measurement discrepancy would clarified future experiment accurate experiment still confirms value SELEX collaboration must unknown mechanism explored: 0.1000\n"
     ]
    }
   ],
   "source": [
    "word_count = Counter(words)  # Count frequency of each word\n",
    "total_words = sum(word_count.values())  # Total number of words\n",
    "average_frequency = {word: count / total_words for word, count in word_count.items()}  # Compute average frequency\n",
    "\n",
    "# Print average frequencies\n",
    "print(\"Average Frequency of Each Word in the Stream:\")\n",
    "for word, freq in average_frequency.items():\n",
    "    print(f\"{word}: {freq:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08c0fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbcc81b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m word_to_vector \u001b[38;5;241m=\u001b[39m {word: vectors[i] \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words)}\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Step 2: Create and populate the LSH structure\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m lsh \u001b[38;5;241m=\u001b[39m LSH(num_hash_tables\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700000\u001b[39m, vector_size\u001b[38;5;241m=\u001b[39m\u001b[43mvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Assuming 5 hash tables and vector dimension size\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Insert words into LSH hash tables\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, vector \u001b[38;5;129;01min\u001b[39;00m word_to_vector\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Function to read the file and store article ID, word, and vectors\n",
    "def read_words(file_path):\n",
    "    article_id = []\n",
    "    words = []\n",
    "    vectors = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 4:  # Assuming each line has article_id, word, date, and vector features\n",
    "                article_id.append(parts[0])\n",
    "                words.append(parts[1])\n",
    "                # Convert the vector string into a numpy array\n",
    "                vector = np.array([float(x) for x in parts[3].split(\",\")])\n",
    "                vectors.append(vector)\n",
    "    return article_id, words, np.array(vectors)\n",
    "\n",
    "# Step 1: Define LSH class to manage hash tables\n",
    "class LSH:\n",
    "    def __init__(self, num_hash_tables, vector_size):\n",
    "        self.num_hash_tables = num_hash_tables\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(num_hash_tables)]\n",
    "        self.random_vectors = [np.random.randn(vector_size) for _ in range(num_hash_tables)]\n",
    "\n",
    "    def hash_vector(self, vector):\n",
    "        # Compute the hash for each hash table using random vectors\n",
    "        return tuple((np.dot(vector, rv) >= 0) for rv in self.random_vectors)\n",
    "\n",
    "    def insert(self, word, vector):\n",
    "        # Insert the word into each hash table based on its hash\n",
    "        for table, rv in zip(self.hash_tables, self.random_vectors):\n",
    "            hash_value = np.dot(vector, rv) >= 0\n",
    "            table[hash_value].append(word)\n",
    "\n",
    "    def query(self, vector, words, k=3):\n",
    "        # Find candidate words in the same hash buckets\n",
    "        candidates = set()\n",
    "        for table, rv in zip(self.hash_tables, self.random_vectors):\n",
    "            hash_value = np.dot(vector, rv) >= 0\n",
    "            candidates.update(table[hash_value])\n",
    "        \n",
    "        # Calculate cosine similarity for the candidates and return top-k results\n",
    "        return sorted([(word, cosine(vector, words[word])) for word in candidates], key=lambda x: x[1])[:k]\n",
    "\n",
    "# Read data from file and initialize LSH\n",
    "article_ids, words, vectors = read_words(\"test.txt\")\n",
    "word_to_vector = {word: vectors[i] for i, word in enumerate(words)}\n",
    "\n",
    "# Step 2: Create and populate the LSH structure\n",
    "lsh = LSH(num_hash_tables=700000, vector_size=vectors.shape[1])  # Assuming 5 hash tables and vector dimension size\n",
    "\n",
    "# Insert words into LSH hash tables\n",
    "for word, vector in word_to_vector.items():\n",
    "    lsh.insert(word, vector)\n",
    "\n",
    "# Step 3: Query with LSH\n",
    "query_word = \"example\"  # Replace with the word you want to search for\n",
    "if query_word in word_to_vector:\n",
    "    query_vector = word_to_vector[query_word]\n",
    "    neighbors = lsh.query(query_vector, word_to_vector, k=3)\n",
    "    print(f\"Top-3 nearest neighbors to '{query_word}':\")\n",
    "    for neighbor, similarity in neighbors:\n",
    "        print(f\"Word: {neighbor}, Similarity: {similarity:.4f}\")\n",
    "else:\n",
    "    print(f\"'{query_word}' not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828fca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
